version: '3.8'

services:
  # LiteLLM with fixed port mapping
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm
    restart: unless-stopped
    ports:
      - "0.0.0.0:8081:4000"  # Map container port 4000 to all interfaces on host port 8081
    volumes:
      - ./configs/litellm_config.yaml:/app/config.yaml
    environment:
      - PORT=4000  # Set to use internal port 4000
      - HOST=0.0.0.0
      - CONFIG_FILE_PATH=/app/config.yaml
    depends_on:
      - redis
    networks:
      - ai-dev-network
    mem_limit: 512m
    
  # Nginx Proxy with updated configuration
  nginx-proxy:
    image: nginx:alpine
    container_name: nginx-proxy
    restart: unless-stopped
    ports:
      - "8082:80"  # Expose nginx on port 8082
    volumes:
      - ./nginx-both-formats:/etc/nginx/conf.d/default.conf
    command: sh -c "nginx -t && nginx -g 'daemon off;'"
    networks:
      - ai-dev-network
    mem_limit: 128m
    
  # Ollama WebUI with consistent configuration
  ollama-ui:
    image: ghcr.io/ollama-webui/ollama-webui:main
    container_name: ollama-ui
    restart: unless-stopped
    ports:
      - "8083:8080"  # Map to host port 8083
    environment:
      - OLLAMA_API_BASE_URL=http://host.docker.internal:8082
      - WEBUI_SECRET_KEY=mysecretkey123456789012345678901234
      - WEBUI_AUTH=false
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./data/ollama-webui:/app/backend/data
    networks:
      - ai-dev-network
    mem_limit: 512m
    
  # Redis for caching
  redis:
    image: redis:alpine
    container_name: redis
    restart: unless-stopped
    ports:
      - "127.0.0.1:6379:6379"  # Only expose Redis locally for security
    volumes:
      - ./data/redis:/data
    command: redis-server --save 60 1 --loglevel warning
    networks:
      - ai-dev-network
    mem_limit: 256m

networks:
  ai-dev-network:
    driver: bridge

volumes:
  redis_data:
    driver: local
